<!doctype html>
<meta charset=utf-8>
<title>Dimensions of motion</title>
<style>
body {
  margin: 0 auto;
  padding: 30px 20px 50px;
  font-family: sans-serif;
  background: #f4f4f4;
  color: #222;
  max-width: 800px;
}
p {
  line-height: 150%;
  max-width: 45em;
}
h1, h2 {
  font-weight: 200;
}
h2 {
  font-size: 22px;
}
h2 a {
  color: #000;
}

h3 {
  margin-top: 40px;
}
a {
  text-decoration: none;
  color: #22b;
}
a:hover {
  text-decoration: underline;
  color: #22b;
}
a:hover img {
  opacity: .5;
}
.paper img { background: #888; }
.video img {
  border-radius: 6px;
  background: #000;
}
h1 { margin-bottom: 1em;}
h1 span { white-space: nowrap; }
h1 span.conference::before { content: "("; vertical-align:.02em; }
h1 span.conference::after { content: ")"; vertical-align:.02em; }
h1 span.conference {
  font-size: 85%;
  line-height: 2em;
}
h2 span {
  display: inline-block;
  padding-top: 1em;
}
em {
  color: #333;
  font-weight: bold;
  font-style: normal;
}
.teaser {
  max-width: 45em;
  margin: 40px 0 60px 0;
  white-space: nowrap;
  position: relative;
}
.teaser .imgs .full {
  width: 100%;
}
.teaser .imgs .center {
  position: absolute;
  width: 32%;
  top: -3%;
  left: 34%;
}
.teaser .labels span {
  display: inline-block;
  width: 33%;
  text-align: center;
  padding-top: 10px;
}
.bibtex {
  font-family: monospace;
  line-height: 150%;
  background: #fff;
  padding: 10px;
  display: inline-block;
  border-radius: 4px;
}
.bibtex div {
  white-space: pre-wrap;
  padding-left: 11em;
  text-indent: -11em;  
}
.crow > * {
  vertical-align: middle;
  margin-right: 20px;
}
a.img {
  display: inline-block;
}
.links {
  display: inline-block;
  line-height: 150%;
  padding: 8px 0;
}
.links a.pad {
  padding: 0 3px;
}
.examples {
  padding: 10px 0;
}
.row, .toprow { white-space: nowrap; }
.row > img {
  height:150px; width:150px; overflow:hidden; display:inline-block; background:white;
}
.toprow div {
  width: 150px; display:inline-block; vertical-align:top; text-align:center;
}
.toprow {
  padding: 10px 0 10px 0;
  font-weight: bold;
}
sup {
  font-size: 60%;
  line-height: 0;
  vertical-align: .6em;
}
</style>
<h1>Dimensions of motion:<br>Monocular prediction through <span>flow subspaces</span>
<br><span class=conference>3DV 2022 Oral</span></h1>
<h2>
  <a href="http://rsbowen.github.io/">Richard Strong Bowen</a><a href="#fn">*</a><sup>12</sup><br>
  <a href="https://research.google/people/RichardTucker/">Richard Tucker</a><a href="#fn">*</a><sup>1</sup><br>
  <a href="https://www.cs.cornell.edu/rdz/">Ramin Zabih</a><sup>12</sup><br>
  <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a><sup>12</sup><br>
  <span><sup>1 </sup>Google Research</span> <span><sup>2 </sup>Cornell Tech</span>
</h2>

<div class=teaser>
<div class=imgs><img class=full src="images/basis.png"></div>
</div>

<h3>Abstract</h3>
<p>We introduce a way to learn to estimate a scene representation from
a single image by predicting a low-dimensional subspace of optical flow
for each training example, which encompasses the variety of possible
camera and object movement. Supervision is provided by a novel loss which
measures the distance between this predicted flow subspace and an observed
optical flow. This provides a new approach to learning scene representation
tasks, such as monocular depth prediction or instance segmentation, in an
unsupervised fashion using in-the-wild input videos without requiring
camera poses, intrinsics, or an explicit multi-view stereo step. We evaluate
our method in multiple settings, including an indoor depth prediction task
where it achieves comparable performance to recent methods trained with more supervision.
</p>


<h3>Paper</h3>
<div class=crow>
<a class=img href="https://arxiv.org/abs/2112.01502"><img src="images/paper.png" width=300 style="border: 1px solid transparent"></a>
<div class=links>
<em>Dimensions of Motion:<br>Monocular prediction through <span>flow subspaces</em><br><br>
Richard Strong Bowen<a href="#fn">*</a>, Richard Tucker<a href="#fn">*</a>,<br>
Ramin Zabih, Noah Snavely<br><br>
[<a class=pad href="https://arxiv.org/abs/2112.01502">Arxiv</a>]
</div>
</div>

<h3>Video</h3>
<div class=crow>
<a class=img href="https://youtu.be/uashUrAeQdw"><img src="images/vid.png" style="border-radius: 6px; border: 1px solid #444;" width=450></a>
<div class=links>
[<a href="https://youtu.be/uashUrAeQdw">YouTube</a>]
</div>
</div>

<h3>Code</h3>
<p>We have released Tensorflow code for flow basis generation, SVD-based projection and accompanying loss functions, network defintions, and some utilities. <br><br>
 [<a href="https://github.com/google-research/google-research/tree/master/dimensions_of_motion">Github</a>]
</p>
  
<h3>Embedding examples</h3>
<p>Each row of the following table shows an input image (overlaid with a few manually
chosen seed points) and the predicted outputs (disparity and embedding) from our network.
The rightmost column then shows the segmentation induced by coloring each pixel according
to which of the seed points is closest to it in bilateral embedding space.
(See Figures 5–6 and Section 4.2 in the paper.)</p>
<div class=examples>
<div class=toprow>
  <div>Input<br>and seed points</div>
  <div><br>Disparity</div>
  <div>Embedding PCA<br>(dimensions 0–2)</div>
  <div>Embedding PCA<br>(dimensions 3–5)</div>
  <div>Induced<br>segmentation</div>
</div>
<div class=row>
  <img src="images/input-00.png">
  <img src="images/disparity-00.png">
  <img src="images/pca1-00.png">
  <img src="images/pca2-00.png">
  <img src="images/output-00.png">
</div>
<div class=row>
  <img src="images/input-01.png">
  <img src="images/disparity-01.png">
  <img src="images/pca1-01.png">
  <img src="images/pca2-01.png">
  <img src="images/output-01.png">
</div>
<div class=row>
  <img src="images/input-02.png">
  <img src="images/disparity-02.png">
  <img src="images/pca1-02.png">
  <img src="images/pca2-02.png">
  <img src="images/output-02.png">
</div>
<div class=row>
  <img src="images/input-03.png">
  <img src="images/disparity-03.png">
  <img src="images/pca1-03.png">
  <img src="images/pca2-03.png">
  <img src="images/output-03.png">
</div>
<div class=row>
  <img src="images/input-04.png">
  <img src="images/disparity-04.png">
  <img src="images/pca1-04.png">
  <img src="images/pca2-04.png">
  <img src="images/output-04.png">
</div>
</div>
(Images used under Creative Commons license from YouTube user POPtravel.)

<h3>BibTeX</h3>
<div class=bibtex>
<div>@inproceedings{bowen2022dimensions,</div>
<div>  title     = {Dimensions of Motion: Monocular Prediction through Flow Subspaces},</div>
<div>  author    = {Richard Strong Bowen and Richard Tucker and Ramin Zabih and Noah Snavely}, </div>
<div>  booktitle = {Proceedings of the International Conference on {3D} Vision (3DV)}, </div>
<div>  year      = {2022}</div>
<div>}</div>
</div>

<hr>

<sup id='fn'>*equal authorial contribution</sup>
